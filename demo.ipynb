{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental configuration\n",
    "data_dir = '../data/cifar10'\n",
    "batch_size_train = 512\n",
    "batch_size_test = 512\n",
    "learning_rate = 0.01\n",
    "momentum = 0.0 # momentum used in SGD optimizer, defaults to be 0.0\n",
    "epochs = 200\n",
    "stop_criterion = 0.01\n",
    "log_interval = 1\n",
    "num_workers = 4\n",
    "gpu_id = 0\n",
    "device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define the dataloader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NRM  = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "TT   = transforms.ToTensor()\n",
    "transform = transforms.Compose([TT, NRM])\n",
    "train_set = datasets.CIFAR10(root=data_dir, train=True, transform=transform, download=True)\n",
    "test_set = datasets.CIFAR10(root=data_dir, train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size_train, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size_test, shuffle=False, num_workers=num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from resnet import resnet18\n",
    "\n",
    "net = resnet18(num_classes=10).to(device)\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "loss_function = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def model_test(net, eval_loader, loss_function, device):\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0.0\n",
    "    loss_vec = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in eval_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = net(data)\n",
    "            loss = loss_function(output, target)\n",
    "            loss_vec.append(loss.data)\n",
    "            test_loss += loss.sum()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "    loss_vec = torch.hstack(loss_vec)\n",
    "    if loss_vec.is_cuda:\n",
    "        loss_vec = loss_vec.cpu()\n",
    "    return correct.item() / len(eval_loader.dataset), test_loss.item() / len(eval_loader.dataset), loss_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_acc/train_loss: 0.0945/2.4239, test_acc/test_loss: 0.0929/2.3717\n",
      "Epoch 1, train_acc/train_loss: 0.3543/1.7930, test_acc/test_loss: 0.3426/1.7679\n",
      "Epoch 2, train_acc/train_loss: 0.4405/1.5550, test_acc/test_loss: 0.4211/1.5485\n",
      "Epoch 3, train_acc/train_loss: 0.4907/1.4116, test_acc/test_loss: 0.4586/1.4278\n",
      "Epoch 4, train_acc/train_loss: 0.5273/1.3061, test_acc/test_loss: 0.4799/1.3464\n",
      "Epoch 5, train_acc/train_loss: 0.5655/1.2107, test_acc/test_loss: 0.5098/1.2817\n",
      "Epoch 6, train_acc/train_loss: 0.5920/1.1395, test_acc/test_loss: 0.5212/1.2422\n",
      "Epoch 7, train_acc/train_loss: 0.6087/1.0881, test_acc/test_loss: 0.5345/1.2129\n",
      "Epoch 8, train_acc/train_loss: 0.6381/1.0117, test_acc/test_loss: 0.5487/1.1772\n",
      "Epoch 9, train_acc/train_loss: 0.6679/0.9432, test_acc/test_loss: 0.5608/1.1415\n",
      "Epoch 10, train_acc/train_loss: 0.6942/0.8767, test_acc/test_loss: 0.5704/1.1228\n",
      "Epoch 11, train_acc/train_loss: 0.7314/0.7910, test_acc/test_loss: 0.5841/1.0908\n",
      "Epoch 12, train_acc/train_loss: 0.7327/0.7786, test_acc/test_loss: 0.5810/1.1054\n",
      "Epoch 13, train_acc/train_loss: 0.7732/0.6820, test_acc/test_loss: 0.5923/1.0767\n",
      "Epoch 14, train_acc/train_loss: 0.7984/0.6183, test_acc/test_loss: 0.5936/1.0773\n",
      "Epoch 15, train_acc/train_loss: 0.8133/0.5738, test_acc/test_loss: 0.5946/1.0882\n",
      "Epoch 16, train_acc/train_loss: 0.8439/0.5062, test_acc/test_loss: 0.5949/1.0858\n",
      "Epoch 17, train_acc/train_loss: 0.8356/0.5142, test_acc/test_loss: 0.5864/1.1311\n",
      "Epoch 18, train_acc/train_loss: 0.8894/0.3966, test_acc/test_loss: 0.5996/1.1151\n",
      "Epoch 19, train_acc/train_loss: 0.9204/0.3168, test_acc/test_loss: 0.5993/1.1251\n",
      "Epoch 20, train_acc/train_loss: 0.9321/0.2810, test_acc/test_loss: 0.5977/1.1530\n",
      "Epoch 21, train_acc/train_loss: 0.9485/0.2327, test_acc/test_loss: 0.6021/1.1730\n",
      "Epoch 22, train_acc/train_loss: 0.9533/0.2088, test_acc/test_loss: 0.5943/1.2163\n",
      "Epoch 23, train_acc/train_loss: 0.9790/0.1391, test_acc/test_loss: 0.5983/1.2242\n",
      "Epoch 24, train_acc/train_loss: 0.9840/0.1127, test_acc/test_loss: 0.6015/1.2603\n",
      "Epoch 25, train_acc/train_loss: 0.9912/0.0749, test_acc/test_loss: 0.6011/1.2852\n",
      "Epoch 26, train_acc/train_loss: 0.9925/0.0578, test_acc/test_loss: 0.6023/1.3108\n",
      "Epoch 27, train_acc/train_loss: 0.9929/0.0455, test_acc/test_loss: 0.6043/1.3411\n",
      "Epoch 28, train_acc/train_loss: 0.9931/0.0388, test_acc/test_loss: 0.6007/1.3713\n",
      "Epoch 29, train_acc/train_loss: 0.9933/0.0296, test_acc/test_loss: 0.6047/1.3853\n",
      "Epoch 30, train_acc/train_loss: 0.9933/0.0257, test_acc/test_loss: 0.6044/1.4120\n",
      "Epoch 31, train_acc/train_loss: 0.9933/0.0227, test_acc/test_loss: 0.6042/1.4339\n",
      "Epoch 32, train_acc/train_loss: 0.9933/0.0193, test_acc/test_loss: 0.6030/1.4495\n",
      "Epoch 33, train_acc/train_loss: 0.9933/0.0165, test_acc/test_loss: 0.6051/1.4683\n",
      "Epoch 34, train_acc/train_loss: 0.9933/0.0144, test_acc/test_loss: 0.6054/1.4839\n",
      "Epoch 35, train_acc/train_loss: 0.9933/0.0133, test_acc/test_loss: 0.6043/1.5009\n",
      "Epoch 36, train_acc/train_loss: 0.9933/0.0119, test_acc/test_loss: 0.6054/1.5158\n",
      "Epoch 37, train_acc/train_loss: 0.9933/0.0108, test_acc/test_loss: 0.6047/1.5282\n",
      "Epoch 38, train_acc/train_loss: 0.9933/0.0101, test_acc/test_loss: 0.6041/1.5398\n",
      "Epoch 39, train_acc/train_loss: 0.9933/0.0091, test_acc/test_loss: 0.6051/1.5538\n"
     ]
    }
   ],
   "source": [
    "# train a model\n",
    "loss_vec_trace_train = []\n",
    "trace_gap = []\n",
    "for epoch in range(epochs):\n",
    "    if epoch % log_interval == 0:\n",
    "        train_acc, train_loss, train_loss_vec = model_test(net, train_loader, loss_function, device)\n",
    "        test_acc, test_loss, _ = model_test(net, test_loader, loss_function, device)\n",
    "        loss_vec_trace_train.append(train_loss_vec)\n",
    "        trace_gap.append(train_acc-test_acc)\n",
    "        print(f'Epoch {epoch}, train_acc/train_loss: {train_acc:.4f}/{train_loss:.4f}, test_acc/test_loss: {test_acc:.4f}/{test_loss:.4f}')\n",
    "    if train_loss <= stop_criterion:\n",
    "        break\n",
    "    net.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data)\n",
    "        loss = loss_function(output, target).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "loss_vec_trace_train = torch.vstack(loss_vec_trace_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the diameter of the evaluated losses\n",
    "import cyminiball as miniball\n",
    "\n",
    "_, r = miniball.compute(loss_vec_trace_train)\n",
    "diameter = 2.0 * np.sqrt(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the gradients by backpropagating a number of mini-batches\n",
    "from utils import cycle_loader, get_grads, get_param\n",
    "\n",
    "num_components = 50000 # number of parameters used to estimate Hurst parameter \n",
    "len_of_sequence = 1000 # number of mini-batches to generate a stochastic sequence\n",
    "cycle_train_loader = cycle_loader(train_loader)\n",
    "_, tot_param = get_param(net)\n",
    "if tot_param < num_components:\n",
    "    num_components = tot_param\n",
    "ids = torch.randperm(tot_param)[:num_components]\n",
    "grads = []\n",
    "for j, (data, target) in enumerate(cycle_train_loader):\n",
    "    if j == len_of_sequence:\n",
    "        break\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = net(data)\n",
    "    loss = loss_function(output, target).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    grad = get_grads(net)\n",
    "    if grad.is_cuda:\n",
    "        grad = grad.cpu()\n",
    "    grads.append(grad[ids])\n",
    "grads = torch.vstack(grads)\n",
    "if grads.is_cuda:\n",
    "    grads = grads.cpu()\n",
    "grads = grads.double().numpy()\n",
    "grads = grads[:, ~np.isnan(grads).any(axis=0)] # delete unvalid elements in case of nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hurst parameter: 0.1597, time elapsed: 48.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# estimate Hurst parameter\n",
    "import os\n",
    "from datetime import datetime\n",
    "import multiprocessing as mp\n",
    "from hurst import compute_Hc\n",
    "\n",
    "start_t = datetime.now()\n",
    "\n",
    "# here we exploit a parallel trick to accelerate the process\n",
    "global get_hurst\n",
    "def get_hurst(j):\n",
    "    h = compute_Hc(grads[:, j], kind='random_walk')\n",
    "    return h\n",
    "pool = mp.Pool(os.cpu_count())\n",
    "res = [pool.apply_async(get_hurst, args=(j,)) for j in range(grads.shape[1])]\n",
    "res = [p.get() for p in res]\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "end_t = datetime.now()\n",
    "elapsed_sec = (end_t - start_t).total_seconds()\n",
    "\n",
    "# remove the outliers\n",
    "res = np.array(res)\n",
    "res = res[res>0.0]\n",
    "res = res[res<1.0]\n",
    "\n",
    "hurst_index = np.median(res)\n",
    "\n",
    "print(\"Hurst parameter: {:.4f}, time elapsed: {:.2f} seconds\".format(hurst_index, elapsed_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical bound: 0.3924, theoretical bound: 0.7825\n"
     ]
    }
   ],
   "source": [
    "theoretical_bound =  24 * diameter / 50000 * np.sqrt(np.log(4.0) / (hurst_index + 1.0e-10))\n",
    "empirical_bound = np.max(np.abs(trace_gap))\n",
    "print(f'Empirical bound: {empirical_bound:.4f}, theoretical bound: {theoretical_bound:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87b8ba51e272dc4897f40b0e94002b12ab8bf7c27263eeea6054a64b7e5d48bc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144px",
    "left": "1072px",
    "right": "20px",
    "top": "148px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
