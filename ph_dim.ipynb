{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load training hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import DictToClass\n",
    "\n",
    "model_dir = 'runs/resnet56_cifar10_sgd/cosine_lr=5.00e-02_bs=128_wd=5.00e-04_corr-1.0_1500_cat[]_seed=1'\n",
    "# model_dir ='runs/resnet56_cifar10_sgd/cosine_lr=4.00e-02_bs=64_wd=5.00e-04_corr-1.0_-1_cat[]_seed=1'\n",
    "\n",
    "with open(os.path.join(model_dir, 'config.json'), 'r') as f:\n",
    "    args = f.read()\n",
    "args = json.loads(args)\n",
    "args = DictToClass(args)\n",
    "\n",
    "# device = torch.device(f'cuda:{args.gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare training set (shffuled v.s. unshuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from data import cifar_dataset\n",
    "from utils import cycle_loader\n",
    "\n",
    "train_set, test_set = cifar_dataset(data_name=args.data_name, root=args.data_dir, label_corruption=args.label_corruption, example_per_class=args.example_per_class, categories=args.categories)\n",
    "\n",
    "train_loader_no_shuffle = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size_eval, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, drop_last=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size_train, shuffle=True, num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_memory, drop_last=True)\n",
    "train_loader_cycle = cycle_loader(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load pretrained neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchcv.model_provider import get_model as ptcv_get_model\n",
    "\n",
    "net =  ptcv_get_model(args.arch, pretrained=False).to(device)\n",
    "net.load_state_dict(torch.load(os.path.join(model_dir, 'state_dict.pt'), map_location=device))\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# note: using momentum will cause ph_dim always approximately equals 1\n",
    "# note: the authors use vaniall SGD with constant learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.005, momentum=0.0, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Continue training with more steps and collect the weights and training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0, train_loss=0.0060, train_acc=99.9133%\n",
      "iteration=500, train_loss=0.0013, train_acc=100.0000%\n",
      "iteration=1000, train_loss=0.0009, train_acc=100.0000%\n",
      "iteration=1500, train_loss=0.0008, train_acc=100.0000%\n",
      "iteration=2000, train_loss=0.0007, train_acc=100.0000%\n",
      "iteration=2500, train_loss=0.0006, train_acc=100.0000%\n"
     ]
    }
   ],
   "source": [
    "from utils import get_params\n",
    "from utils import validate\n",
    "\n",
    "max_points = 3000 # too large value sometimes will cause training failure\n",
    "train_loss_hist = []\n",
    "weights_hist = []\n",
    "for j, (x, y) in enumerate(train_loader_cycle):\n",
    "    if j == max_points:\n",
    "        break\n",
    "    net.train()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    yhat = net(x)\n",
    "    loss = loss_func(yhat, y)\n",
    "    loss.mean().backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    curr_params = get_params(net)\n",
    "    weights_hist.append(curr_params)\n",
    "    \n",
    "    # train_loss, train_acc, train_loss_vec = validate(net, train_loader_no_shuffle, loss_func, device, train=False)\n",
    "    # train_loss_hist.append(train_loss_vec)\n",
    "    if j % 500 == 0:\n",
    "        train_loss, train_acc, train_loss_vec = validate(net, train_loader_no_shuffle, loss_func, device, train=False)\n",
    "        print(f'iteration={j}, train_loss={train_loss:.4f}, train_acc={train_acc*100:.4f}%')\n",
    "weights_hist = torch.stack(weights_hist, dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute persistent homology dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-06 18:02:25.725\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mindicator.topology\u001b[0m:\u001b[36mfast_ripser\u001b[0m:\u001b[36m84\u001b[0m - \u001b[34m\u001b[1mDistance matrix computation time: 131.51s\u001b[0m\n",
      "\u001b[32m2024-07-06 18:02:45.876\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mindicator.topology\u001b[0m:\u001b[36mph_dim_from_distance_matrix\u001b[0m:\u001b[36m64\u001b[0m - \u001b[34m\u001b[1mPh Dimension Calculation has an approximate error of: 0.0004687931896550983.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PH dimension 1.6849122710899136\n"
     ]
    }
   ],
   "source": [
    "from indicator import fast_ripser\n",
    "ph_dim_euclidean = fast_ripser(weights_hist, max_points=max_points, min_points=200, point_jump=20)\n",
    "\n",
    "print('PH dimension', ph_dim_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
