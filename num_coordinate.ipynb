{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load training hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import DictToClass\n",
    "\n",
    "model_dir = 'runs/resnet20_cifar10_sgd/cosine_lr=1.00e-01_bs=128_wd=5.00e-04_corr-1.0_1000_cat[]_seed=1'\n",
    "with open(os.path.join(model_dir, 'config.json'), 'r') as f:\n",
    "    args = f.read()\n",
    "args = json.loads(args)\n",
    "args = DictToClass(args)\n",
    "\n",
    "# device = torch.device(f'cuda:{args.gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare training set (shffuled v.s. unshuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from data import cifar_dataset\n",
    "from utils import cycle_loader\n",
    "\n",
    "train_set, test_set = cifar_dataset(data_name=args.data_name, root=args.data_dir, label_corruption=args.label_corruption, example_per_class=args.example_per_class, categories=args.categories)\n",
    "\n",
    "train_loader_no_shuffle = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size_train, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, drop_last=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size_train, shuffle=True, num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_memory, drop_last=True)\n",
    "train_loader_cycle = cycle_loader(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load pretrained neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchcv.model_provider import get_model as ptcv_get_model\n",
    "\n",
    "net =  ptcv_get_model(args.arch, pretrained=False).to(device)\n",
    "net.load_state_dict(torch.load(os.path.join(model_dir, 'state_dict.pt'), map_location=device))\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculate true gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_grads\n",
    "\n",
    "true_gradient = 0.0\n",
    "net.train()\n",
    "for x, y in train_loader_no_shuffle:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    net.zero_grad()\n",
    "    yhat = net(x)\n",
    "    loss = loss_func(yhat, y)\n",
    "    loss.mean().backward()\n",
    "    curr_gradient = get_grads(net)\n",
    "    true_gradient = true_gradient + curr_gradient * len(x)\n",
    "true_gradient = true_gradient / len(train_loader_no_shuffle.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estimate a sequence of stochastic gradient noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid number of stochastic gradient noise sequence:  270906\n"
     ]
    }
   ],
   "source": [
    "num_components = 300000 # number of coordinates used to estimate Hurst parameter \n",
    "len_of_sequence = 3000 # number of mini-batches to generate a stochastic sequence\n",
    "tot_param = len(true_gradient)        \n",
    "if tot_param < num_components:\n",
    "    num_components = tot_param\n",
    "fixed_dims = torch.randperm(tot_param)[:num_components]\n",
    "\n",
    "stochastic_grads = []\n",
    "\n",
    "net.train()\n",
    "for j, (x, y) in enumerate(train_loader_cycle):\n",
    "    if j == len_of_sequence:\n",
    "        break\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    net.zero_grad()\n",
    "    yhat = net(x)\n",
    "    loss = loss_func(yhat, y)\n",
    "    loss.mean().backward()\n",
    "    curr_gradient = get_grads(net)\n",
    "    stochastic_grads.append(curr_gradient[fixed_dims])\n",
    "stochastic_grads = torch.vstack(stochastic_grads)\n",
    "\n",
    "stochastic_grads = stochastic_grads - true_gradient[fixed_dims]\n",
    "\n",
    "stochastic_grads = stochastic_grads.double().cpu().numpy()\n",
    "stochastic_grads = stochastic_grads[:, ~np.isnan(stochastic_grads).any(axis=0)] # delete unvalid elements in case of nan\n",
    "\n",
    "print('valid number of stochastic gradient noise sequence: ', stochastic_grads.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estimate Hurst exponent for each coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average Hurst exponent: 0.46\n",
      "elapsed time: 64.73 seconds\n",
      "smallest ten exponents:  [0.31281962 0.32537243 0.32538306 0.32541635 0.32877717 0.33035786\n",
      " 0.33155275 0.33155303 0.33157645 0.33158003]\n"
     ]
    }
   ],
   "source": [
    "# here we exploit a parallel trick to accelerate the process\n",
    "import nolds\n",
    "from datetime import datetime\n",
    "from multiprocessing import get_context\n",
    "from utils import error_func, calc_hurst_exponent\n",
    "from utils import fractal_dimension\n",
    "\n",
    "start_t = datetime.now()\n",
    "with get_context(\"spawn\").Pool(min(os.cpu_count(), 36)) as pool:\n",
    "    exponents = [pool.apply_async(calc_hurst_exponent, args=(stochastic_grads[:, j],), error_callback=error_func) for j in range(stochastic_grads.shape[1])]\n",
    "    exponents = [p.get() for p in exponents]\n",
    "# remove the failures\n",
    "exponents = np.array(exponents)\n",
    "exponents = exponents[exponents >= 0.01]\n",
    "exponents = exponents[exponents <= 0.99]\n",
    "dim_h = fractal_dimension(exponents)\n",
    "end_t = datetime.now()\n",
    "elapsed_sec = (end_t - start_t).total_seconds()\n",
    "\n",
    "print(f'hausdorff dimension: {dim_h:.2f}, average Hurst exponent: {np.mean(exponents):.2f}')\n",
    "print(f'elapsed time: {elapsed_sec:.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
