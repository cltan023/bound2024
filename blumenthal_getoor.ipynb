{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load training hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import DictToClass\n",
    "\n",
    "model_dir = 'runs/resnet56_cifar10_sgd/cosine_lr=5.00e-02_bs=128_wd=5.00e-04_corr-1.0_1000_cat[]_seed=1'\n",
    "# model_dir ='runs/resnet56_cifar10_sgd/cosine_lr=4.00e-02_bs=1024_wd=5.00e-04_corr-1.0_-1_cat[]_seed=1'\n",
    "\n",
    "with open(os.path.join(model_dir, 'config.json'), 'r') as f:\n",
    "    args = f.read()\n",
    "args = json.loads(args)\n",
    "args = DictToClass(args)\n",
    "\n",
    "# device = torch.device(f'cuda:{args.gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare unshuffled training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from data import cifar_dataset\n",
    "from utils import cycle_loader\n",
    "\n",
    "train_set, test_set = cifar_dataset(data_name=args.data_name, root=args.data_dir, label_corruption=args.label_corruption, example_per_class=args.example_per_class, categories=args.categories)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size_train, shuffle=True, num_workers=args.num_workers,\n",
    "    pin_memory=args.pin_memory, drop_last=True)\n",
    "\n",
    "train_loader_cycle = cycle_loader(train_loader)\n",
    "\n",
    "# train_loader_no_shuffle = torch.utils.data.DataLoader(train_set, batch_size=args.batch_size_eval, shuffle=False, num_workers=args.num_workers, pin_memory=args.pin_memory, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load pretrained neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchcv.model_provider import get_model as ptcv_get_model\n",
    "\n",
    "net =  ptcv_get_model(args.arch, pretrained=False).to(device)\n",
    "net.load_state_dict(torch.load(os.path.join(model_dir, 'state_dict.pt'), map_location=device))\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# note: the authors of Sim¸sekli, 2020 use vaniall SGD with constant learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.0, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Continue training with one more epoch and collect the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicator import peek_model_size\n",
    "from utils import validate\n",
    "\n",
    "ms = peek_model_size(net)\n",
    "iter_num = 391 # number of mini-batches in one epoch\n",
    "parameter_arrays = []\n",
    "for mod_size in ms:\n",
    "    # print(\"ozan\", iter_num, mod_size)\n",
    "    parameter_arrays.append(torch.zeros(iter_num, mod_size))\n",
    "\n",
    "for j, (x, y) in enumerate(train_loader_cycle):\n",
    "    if j == iter_num:\n",
    "        break\n",
    "    net.train()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    yhat = net(x)\n",
    "    loss = loss_func(yhat, y).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    read_mem_cnt = 0\n",
    "    for p in net.parameters():\n",
    "        cpu_data = p.data.cpu().view(1,-1)\n",
    "        if len(p.shape) < 2:\n",
    "            continue\n",
    "        parameter_arrays[read_mem_cnt][j, 0:np.prod(p.shape)] = cpu_data\n",
    "        read_mem_cnt +=1\n",
    "        \n",
    "    if j % 50 == 0:\n",
    "        # train_loss, train_acc, train_loss_vec = validate(net, train_loader_no_shuffle, loss_func, device, train=False)\n",
    "        # print(f'iteration={j}, train_loss={train_loss:.4f}, train_acc={train_acc*100:.4f}%')\n",
    "        print(f'iteration={j}, train_loss={loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute Blumenthal-Getoor index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean value: 0.9970, max value: 1.0357\n"
     ]
    }
   ],
   "source": [
    "from indicator import estimator_scalar, estimator_vector_full, estimator_vector_mean, estimator_vector_projected\n",
    "\n",
    "# All models are stored in the memory, so we need to call estimators\n",
    "# Following Sim¸sekli, 2020, we only use estimator_vector_projected function, others are included for future research\n",
    "# alpha_full_est = []\n",
    "alpha_proj_med_est = []\n",
    "alpha_proj_max_est = []\n",
    "# alpha_mean_est = []\n",
    "# alpha_scalar_est = []\n",
    "for param in parameter_arrays:\n",
    "    # alpha_full = estimator_vector_full(param)\n",
    "    # alpha_full_est.append(alpha_full)\n",
    "\n",
    "    alpha_proj_med, alpha_proj_max = estimator_vector_projected(param)\n",
    "    alpha_proj_med_est.append(alpha_proj_med)\n",
    "    alpha_proj_max_est.append(alpha_proj_max)\n",
    "\n",
    "    # alpha_mean = estimator_vector_mean(param)\n",
    "    # alpha_mean_est.append(alpha_mean)\n",
    "\n",
    "    # alpha_scalar = estimator_scalar(param)\n",
    "    # alpha_scalar_est.append(alpha_scalar) \n",
    "\n",
    "print(f'mean value: {np.mean(alpha_proj_med_est):.4f}, max value: {np.mean(alpha_proj_max_est):.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
